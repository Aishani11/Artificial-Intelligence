{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b2fe84e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\aisha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#importing dependencies\n",
    "import numpy\n",
    "import sys\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import ModelCheckpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70de429a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data\n",
    "#loading data and opening our input data in the form of a txt file\n",
    "#Project Gutenburg is where the data can be found\n",
    "file = open(r\"C:\\Users\\aisha\\Downloads\\gutenberg_metadata.txt\",encoding=\"utf8\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2211ff2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenization\n",
    "#standardization\n",
    "def tokenize_words(input):\n",
    "    #lowercase everything to standardize it\n",
    "    input = input.lower()\n",
    "    #instanting the tokenizer\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    #tokenizing the texts into tokens\n",
    "    tokens = tokenizer.tokenize(input)\n",
    "    #filtering the stopwords using lambda\n",
    "    stop_words = set(stopwords.words('english')) \n",
    "    # Use words() method to get stopwords\n",
    "    filtered = [token for token in tokens if token not in stop_words]\n",
    "    return \" \".join(filtered)  # Use space to join the tokens\n",
    "\n",
    "#preprocess the input data, make tokens\n",
    "processed_inputs = tokenize_words(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "112d80e6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#chars to numbers\n",
    "#convert characters in our input to numbers\n",
    "#we'll sort the list of the set of all characters that appear in our i/p text and then use the enumerate fn\n",
    "#to get numbers that represent the characters\n",
    "#we'll then create a dictionary that stores the keys and values, or the numbers and characters that representthem\n",
    "\n",
    "chars = sorted(list(set(processed_inputs)))\n",
    "char_to_num = dict((c,i) for i,c in enumerate(chars))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80554688",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of characters: 1233402\n",
      "Total number of vocab: 122\n"
     ]
    }
   ],
   "source": [
    "#check if words to chars or chars to num has worked (?!) has worked ?\n",
    "#print length of our variables\n",
    "input_len = len(processed_inputs)\n",
    "vocab_len = len(chars)\n",
    "print(\"Total number of characters:\", input_len)\n",
    "print(\"Total number of vocab:\" , vocab_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8cbf2d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#seq length\n",
    "#defining how long we want our individual sequence here\n",
    "#an individual sequence is the mapping of input characters as integers\n",
    "seq_length = 100\n",
    "x_data = []\n",
    "y_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ba9d4b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Patterns: 1233302\n"
     ]
    }
   ],
   "source": [
    "#loop throught the sequence\n",
    "#going through the entire list and converting chars to numbers with a for loop\n",
    "#this will create a bunch of sequences where each sequence stars with the next character in the i/p data\n",
    "#beginning with the first character\n",
    "\n",
    "for i in range(0, input_len - seq_length, 1):\n",
    "    #define i/p and o/p sequences\n",
    "    #i/p is the current character plus the desired sequence length\n",
    "    in_seq = processed_inputs[i:i + seq_length]\n",
    "    #o/p is the initial character plus total sequence length\n",
    "    out_seq = processed_inputs[i + seq_length]\n",
    "    # Check if all characters in in_seq exist in char_to_num dictionary\n",
    "    x_data.append([char_to_num[char] for char in in_seq])\n",
    "    y_data.append(char_to_num[out_seq])\n",
    "\n",
    "#check to see how many i/p sequence we have\n",
    "n_patterns = len(x_data)\n",
    "print (\"Total Patterns:\", n_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd54f7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert input sequence to np array and so on\n",
    "X = numpy.reshape(x_data, (n_patterns, seq_length, 1))\n",
    "X = X/float(vocab_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b831401",
   "metadata": {},
   "outputs": [],
   "source": [
    "#one-hot encoding\n",
    "y = np_utils.to_categorical(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "121da924",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the model\n",
    "#creating a sequential model\n",
    "#dropout is used to prevent overfitting\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences = True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(256, return_sequences = True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d5d6319f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fe84145b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving weights\n",
    "filepath = \"model_weights_saved.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose = 1, save_best_only=True, mode='min')\n",
    "desired_callbacks = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7c94ccfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4818/4818 [==============================] - ETA: 0s - loss: 2.2482\n",
      "Epoch 1: loss improved from inf to 2.24822, saving model to model_weights_saved.hdf5\n",
      "4818/4818 [==============================] - 23690s 5s/step - loss: 2.2482\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1e99e6fb8b0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fit model and let it train\n",
    "model.fit(X, y, epochs=1, batch_size=256, callbacks=desired_callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9b61c992",
   "metadata": {},
   "outputs": [],
   "source": [
    "#recompile model with the saved weights\n",
    "filename = \"model_weights_saved.hdf5\"\n",
    "model.load_weights(filename)\n",
    "model.compile(loss='categorical_crossentropy', optimizer ='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e0f466bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#output of the model back into characters\n",
    "num_to_char = dict((i,c) for i,c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b07bf6ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seed:\n",
      "\" lantyne http www gutenberg org ebooks 21711 times peril tale india g henty http www gutenberg org eb \"\n"
     ]
    }
   ],
   "source": [
    "#random seed to help generate \n",
    "start = numpy.random.randint(0, len(x_data)-1)\n",
    "pattern = x_data[start]\n",
    "print(\"Random Seed:\")\n",
    "print(\"\\\"\",''.join([num_to_char[value] for value in pattern]), \"\\\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d6d1cb09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ooks 1111  sore sare sare hart http www gutenberg org ebooks 1111  sore sare sare hart http www gutenberg org ebooks 1111  sore sare sare hart http www gutenberg org ebooks 1111  sore sare sare hart http www gutenberg org ebooks 1111  sore sare sare hart http www gutenberg org ebooks 1111  sore sare sare hart http www gutenberg org ebooks 1111  sore sare sare hart http www gutenberg org ebooks 1111  sore sare sare hart http www gutenberg org ebooks 1111  sore sare sare hart http www gutenberg org ebooks 1111  sore sare sare hart http www gutenberg org ebooks 1111  sore sare sare hart http www gutenberg org ebooks 1111  sore sare sare hart http www gutenberg org ebooks 1111  sore sare sare hart http www gutenberg org ebooks 1111  sore sare sare hart http www gutenberg org ebooks 1111  sore sare sare hart http www gutenberg org ebooks 1111  sore sare sare hart http www gutenberg org ebooks 1111  sore sare sare hart http www gutenberg org ebooks 1111  sore sare sare hart http www gutenber"
     ]
    }
   ],
   "source": [
    "#generate the text\n",
    "for i in range(1000):\n",
    "    x = numpy.reshape(pattern, (1,len(pattern),1))\n",
    "    x = x/float(vocab_len)\n",
    "    prediction = model.predict(x,verbose=0)\n",
    "    index = numpy.argmax(prediction)\n",
    "    result = num_to_char[index]\n",
    "    seq_in = [num_to_char[value] for value in pattern ]\n",
    "    sys.stdout.write(result)\n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1:len(pattern)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
